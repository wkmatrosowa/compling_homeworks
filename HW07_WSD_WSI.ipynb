{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "HW07_WSD_WSI.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wksmirnowa/compling_homeworks/blob/master/HW07_WSD_WSI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFW5S7uHJgt3",
        "colab_type": "text"
      },
      "source": [
        "# Word Sense Disambiguation (снятие лексической неоднозначности) и Word Sense Induction (нахождение значений слова)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwvXP47-M53u",
        "colab_type": "text"
      },
      "source": [
        "## Подготовка: импорты и предобработка"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4Gg-RkVMWMJ",
        "colab_type": "code",
        "outputId": "62958e04-31d3-4b22-fd43-0856001d639f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        }
      },
      "source": [
        "!pip install pymorphy2[fast]\n",
        "!pip install Cython numpy\n",
        "!pip install git+https://github.com/lopuhin/python-adagram.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pymorphy2[fast]\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/33/fff9675c68b5f6c63ec8c6e6ff57827dda28a1fa5b2c2d727dffff92dd47/pymorphy2-0.8-py2.py3-none-any.whl (46kB)\n",
            "\r\u001b[K     |███████                         | 10kB 22.7MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 20kB 3.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 30kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 40kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2[fast]) (0.6.2)\n",
            "Collecting pymorphy2-dicts<3.0,>=2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/51/2465fd4f72328ab50877b54777764d928da8cb15b74e2680fc1bd8cb3173/pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1MB 10.0MB/s \n",
            "\u001b[?25hCollecting dawg-python>=0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
            "Collecting DAWG>=0.7.3; extra == \"fast\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b8/ef/91b619a399685f7a0a95a03628006ba814d96293bbbbed234ee66fbdefd9/DAWG-0.8.0.tar.gz (371kB)\n",
            "\u001b[K     |████████████████████████████████| 378kB 51.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: DAWG\n",
            "  Building wheel for DAWG (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for DAWG: filename=DAWG-0.8.0-cp36-cp36m-linux_x86_64.whl size=853636 sha256=54bf510b905ced3bbdc40e438e9eaae3abb067d3576677572f4b65b439e8a6e5\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/1f/f0/a5b1f9d02e193c997d252c33d215f24dfd7a448bc0166b2a12\n",
            "Successfully built DAWG\n",
            "Installing collected packages: pymorphy2-dicts, dawg-python, DAWG, pymorphy2\n",
            "Successfully installed DAWG-0.8.0 dawg-python-0.7.2 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (0.29.17)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.4)\n",
            "Collecting git+https://github.com/lopuhin/python-adagram.git\n",
            "  Cloning https://github.com/lopuhin/python-adagram.git to /tmp/pip-req-build-49naxx1_\n",
            "  Running command git clone -q https://github.com/lopuhin/python-adagram.git /tmp/pip-req-build-49naxx1_\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from adagram==0.0.1) (0.29.17)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from adagram==0.0.1) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.9 in /usr/local/lib/python3.6/dist-packages (from adagram==0.0.1) (1.18.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from adagram==0.0.1) (1.12.0)\n",
            "Building wheels for collected packages: adagram\n",
            "  Building wheel for adagram (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for adagram: filename=adagram-0.0.1-cp36-cp36m-linux_x86_64.whl size=464988 sha256=5060b7b4d0e18b3c5b5a4d2a47698f54dd6132929825c5546a29694ff916e806\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-urf7sao2/wheels/11/0f/46/f5df96670df8f7973b4c2311ffc9b02e435a7bd3207f992c4d\n",
            "Successfully built adagram\n",
            "Installing collected packages: adagram\n",
            "Successfully installed adagram-0.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2QJu4meJgt-",
        "colab_type": "code",
        "outputId": "a79e5109-e6bc-4c9c-d089-061ee8e34c88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "import adagram\n",
        "from lxml import html\n",
        "import sys\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import pandas as pd\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from string import punctuation\n",
        "import json, os\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "morph = MorphAnalyzer()\n",
        "lem = WordNetLemmatizer()\n",
        "\n",
        "punct = punctuation+'«»—…“”*№–'\n",
        "stops = set(stopwords.words('russian')) | {'gt', }\n",
        "added_stops = {'весь', 'это', 'наш', 'оно', 'итак', 'т.п', 'т.е', 'мало', 'меньше', 'ещё', 'слишком', 'также',\n",
        "                   'ваш', 'б', 'хм', 'который', 'свой', 'не', 'мочь', 'однако', 'очень', 'благодаря', 'кроме'}\n",
        "stops = stops.union(added_stops)\n",
        "\n",
        "\n",
        "def tokenize(text):\n",
        "    \n",
        "    words = [word.strip(punct) for word in text.lower().split() if word and word not in stops]\n",
        "    words = [word for word in words if word]\n",
        "\n",
        "    return words\n",
        "\n",
        "def normalize(text):\n",
        "    \n",
        "    words = tokenize(text)\n",
        "    words = [morph.parse(word)[0].normal_form for word in words if word]\n",
        "\n",
        "    return words\n",
        "\n",
        "def normalize_en(text):\n",
        "\n",
        "  words = []\n",
        "  for word, tag in pos_tag(text):\n",
        "    lemtag = tag[0].lower()\n",
        "    lemtag = lemtag if lemtag in ['a', 'r', 'n', 'v'] else None\n",
        "\n",
        "    if not lemtag:\n",
        "      lemma = word\n",
        "      words.append(lemma)\n",
        "    else:\n",
        "      lemma = lem.lemmatize(word, lemtag)\n",
        "      words.append(lemma)\n",
        "\n",
        "  return words\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1IM16vYMipd",
        "colab_type": "code",
        "outputId": "f5fbe5d4-226b-4008-b3b9-15ffbf1b9bc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDWS0f9iM_jh",
        "colab_type": "text"
      },
      "source": [
        "# Задание 1.\n",
        "\n",
        "## Подготовка"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rM4g0gV8PE-I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Корпус гуманитарных текстов из кругосвета для обучения Адаграма\n",
        "\n",
        "# corpus_hum = open('/content/drive/My Drive/corpus_hum.txt').read().splitlines()\n",
        "# corpus_hum_norm = [normalize(text) for text in corpus_hum]\n",
        "# corpus_hum_norm = [text for text in corpus_hum_norm if text]\n",
        "# f = open('corpus_hum.txt', 'w')\n",
        "# for element in corpus_hum_norm:\n",
        "#   f.write(' '.join(element))\n",
        "# f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRPuPDGTM3fr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Создание корпуса перефразирования \n",
        "\n",
        "corpus_xml = html.fromstring(open('/content/drive/My Drive/paraphraser/paraphrases.xml', 'rb').read())\n",
        "texts_1 = []\n",
        "texts_2 = []\n",
        "classes = []\n",
        "\n",
        "for p in corpus_xml.xpath('//paraphrase'):\n",
        "    texts_1.append(p.xpath('./value[@name=\"text_1\"]/text()')[0])\n",
        "    texts_2.append(p.xpath('./value[@name=\"text_2\"]/text()')[0])\n",
        "    classes.append(p.xpath('./value[@name=\"class\"]/text()')[0])\n",
        "    \n",
        "data = pd.DataFrame({'text_1':texts_1, 'text_2':texts_2, 'label':classes})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPoAIf0YspJI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['text_1_norm'] = data['text_1'].apply(normalize)\n",
        "data['text_2_norm'] = data['text_2'].apply(normalize)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca6_J0juNYUq",
        "colab_type": "text"
      },
      "source": [
        "## Обучение Адаграма"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlLDK-q4NXeL",
        "colab_type": "code",
        "outputId": "a0ae1ee6-9e7b-4450-aed8-9892b78676eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time \n",
        "!adagram-train corpus_hum.txt out.pkl"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] 2020-04-30 08:49:10,998 Building dictionary...\n",
            "[INFO] 2020-04-30 08:55:22,098 Done! 27414 words.\n",
            "[INFO] 2020-04-30 09:01:35,385 1.74% -8.6608 0.0246 1.2/2.0 0.17 kwords/sec\n",
            "[INFO] 2020-04-30 09:01:42,349 3.49% -8.5366 0.0241 1.2/2.0 9.19 kwords/sec\n",
            "[INFO] 2020-04-30 09:01:49,198 5.23% -8.4243 0.0237 1.2/2.0 9.34 kwords/sec\n",
            "[INFO] 2020-04-30 09:01:55,924 6.98% -8.3197 0.0233 1.2/2.0 9.52 kwords/sec\n",
            "[INFO] 2020-04-30 09:02:02,498 8.72% -8.2186 0.0228 1.2/2.0 9.74 kwords/sec\n",
            "[INFO] 2020-04-30 09:02:08,885 10.46% -8.1190 0.0224 1.2/2.0 10.02 kwords/sec\n",
            "[INFO] 2020-04-30 09:02:15,103 12.21% -8.0209 0.0219 1.2/3.0 10.29 kwords/sec\n",
            "[INFO] 2020-04-30 09:02:21,282 13.95% -7.9249 0.0215 1.2/3.0 10.36 kwords/sec\n",
            "[INFO] 2020-04-30 09:02:27,424 15.70% -7.8322 0.0211 1.2/4.0 10.42 kwords/sec\n",
            "[INFO] 2020-04-30 09:02:33,554 17.44% -7.7435 0.0206 1.3/4.0 10.44 kwords/sec\n",
            "[INFO] 2020-04-30 09:02:39,675 19.18% -7.6593 0.0202 1.3/4.0 10.46 kwords/sec\n",
            "[INFO] 2020-04-30 09:02:45,780 20.93% -7.5801 0.0198 1.3/4.0 10.48 kwords/sec\n",
            "[INFO] 2020-04-30 09:02:51,891 22.67% -7.5057 0.0193 1.3/4.0 10.47 kwords/sec\n",
            "[INFO] 2020-04-30 09:02:57,971 24.42% -7.4362 0.0189 1.3/5.0 10.53 kwords/sec\n",
            "[INFO] 2020-04-30 09:03:04,068 26.16% -7.3713 0.0185 1.3/5.0 10.50 kwords/sec\n",
            "[INFO] 2020-04-30 09:03:10,150 27.90% -7.3107 0.0180 1.3/5.0 10.52 kwords/sec\n",
            "[INFO] 2020-04-30 09:03:16,228 29.65% -7.2541 0.0176 1.3/5.0 10.53 kwords/sec\n",
            "[INFO] 2020-04-30 09:03:22,299 31.39% -7.2012 0.0172 1.3/5.0 10.54 kwords/sec\n",
            "[INFO] 2020-04-30 09:03:28,341 33.14% -7.1518 0.0167 1.3/5.0 10.59 kwords/sec\n",
            "[INFO] 2020-04-30 09:03:34,368 34.88% -7.1055 0.0163 1.3/5.0 10.62 kwords/sec\n",
            "[INFO] 2020-04-30 09:03:40,396 36.62% -7.0621 0.0158 1.3/5.0 10.62 kwords/sec\n",
            "[INFO] 2020-04-30 09:03:46,413 38.37% -7.0214 0.0154 1.3/5.0 10.64 kwords/sec\n",
            "[INFO] 2020-04-30 09:03:52,419 40.11% -6.9830 0.0150 1.3/5.0 10.66 kwords/sec\n",
            "[INFO] 2020-04-30 09:03:58,409 41.86% -6.9468 0.0145 1.3/5.0 10.68 kwords/sec\n",
            "[INFO] 2020-04-30 09:04:04,406 43.60% -6.9127 0.0141 1.3/5.0 10.67 kwords/sec\n",
            "[INFO] 2020-04-30 09:04:10,374 45.34% -6.8804 0.0137 1.3/5.0 10.72 kwords/sec\n",
            "[INFO] 2020-04-30 09:04:16,347 47.09% -6.8499 0.0132 1.3/5.0 10.71 kwords/sec\n",
            "[INFO] 2020-04-30 09:04:22,334 48.83% -6.8209 0.0128 1.3/5.0 10.69 kwords/sec\n",
            "[INFO] 2020-04-30 09:04:28,287 50.58% -6.7933 0.0124 1.3/5.0 10.75 kwords/sec\n",
            "[INFO] 2020-04-30 09:04:34,224 52.32% -6.7671 0.0119 1.3/5.0 10.78 kwords/sec\n",
            "[INFO] 2020-04-30 09:04:40,194 54.06% -6.7421 0.0115 1.3/5.0 10.72 kwords/sec\n",
            "[INFO] 2020-04-30 09:04:46,132 55.81% -6.7183 0.0110 1.3/5.0 10.78 kwords/sec\n",
            "[INFO] 2020-04-30 09:04:52,078 57.55% -6.6956 0.0106 1.3/5.0 10.76 kwords/sec\n",
            "[INFO] 2020-04-30 09:04:58,009 59.30% -6.6738 0.0102 1.3/5.0 10.79 kwords/sec\n",
            "[INFO] 2020-04-30 09:05:03,928 61.04% -6.6530 0.0097 1.3/5.0 10.81 kwords/sec\n",
            "[INFO] 2020-04-30 09:05:09,852 62.78% -6.6330 0.0093 1.3/5.0 10.80 kwords/sec\n",
            "[INFO] 2020-04-30 09:05:15,760 64.53% -6.6139 0.0089 1.3/5.0 10.83 kwords/sec\n",
            "[INFO] 2020-04-30 09:05:21,686 66.27% -6.5955 0.0084 1.3/5.0 10.80 kwords/sec\n",
            "[INFO] 2020-04-30 09:05:27,581 68.02% -6.5778 0.0080 1.3/5.0 10.86 kwords/sec\n",
            "[INFO] 2020-04-30 09:05:33,487 69.76% -6.5608 0.0076 1.3/5.0 10.84 kwords/sec\n",
            "[INFO] 2020-04-30 09:05:39,400 71.50% -6.5444 0.0071 1.3/5.0 10.82 kwords/sec\n",
            "[INFO] 2020-04-30 09:05:45,299 73.25% -6.5287 0.0067 1.3/5.0 10.85 kwords/sec\n",
            "[INFO] 2020-04-30 09:05:51,203 74.99% -6.5134 0.0063 1.3/5.0 10.84 kwords/sec\n",
            "[INFO] 2020-04-30 09:05:57,092 76.74% -6.4988 0.0058 1.3/5.0 10.87 kwords/sec\n",
            "[INFO] 2020-04-30 09:06:02,999 78.48% -6.4846 0.0054 1.3/5.0 10.83 kwords/sec\n",
            "[INFO] 2020-04-30 09:06:08,917 80.22% -6.4709 0.0049 1.3/5.0 10.82 kwords/sec\n",
            "[INFO] 2020-04-30 09:06:14,827 81.97% -6.4577 0.0045 1.3/5.0 10.83 kwords/sec\n",
            "[INFO] 2020-04-30 09:06:20,730 83.71% -6.4450 0.0041 1.3/5.0 10.84 kwords/sec\n",
            "[INFO] 2020-04-30 09:06:26,594 85.46% -6.4326 0.0036 1.3/5.0 10.92 kwords/sec\n",
            "[INFO] 2020-04-30 09:06:32,470 87.20% -6.4207 0.0032 1.3/5.0 10.89 kwords/sec\n",
            "[INFO] 2020-04-30 09:06:38,340 88.94% -6.4092 0.0028 1.3/5.0 10.90 kwords/sec\n",
            "[INFO] 2020-04-30 09:06:44,242 90.69% -6.3981 0.0023 1.3/5.0 10.84 kwords/sec\n",
            "[INFO] 2020-04-30 09:06:50,121 92.43% -6.3874 0.0019 1.3/5.0 10.89 kwords/sec\n",
            "[INFO] 2020-04-30 09:06:55,975 94.18% -6.3771 0.0015 1.3/5.0 10.93 kwords/sec\n",
            "[INFO] 2020-04-30 09:07:01,835 95.92% -6.3672 0.0010 1.3/5.0 10.92 kwords/sec\n",
            "[INFO] 2020-04-30 09:07:07,680 97.66% -6.3576 0.0006 1.3/5.0 10.95 kwords/sec\n",
            "[INFO] 2020-04-30 09:07:13,538 99.41% -6.3485 0.0001 1.3/5.0 10.92 kwords/sec\n",
            "[INFO] 2020-04-30 09:07:15,522 100.00% -6.3456 0.0000 1.3/5.0 10.97 kwords/sec\n",
            "CPU times: user 2.67 s, sys: 380 ms, total: 3.05 s\n",
            "Wall time: 18min 6s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mx0iDV0wONE9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Загрузить модель, обученную на текстах из кругосвета\n",
        "# adagram_model = adagram.VectorModel.load(\"/content/drive/My Drive/out.pkl\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPEZeTtkXh4i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Загрузить обученную модель с семинара\n",
        "adagram_model = adagram.VectorModel.load(\"/content/drive/My Drive/wsd_wsi_seminar/out.pkl\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klbHyVV_iESM",
        "colab_type": "text"
      },
      "source": [
        "Векторизуйте пары текстов с помощью Адаграма. За основу возьмите код из предыдущего семинара/домашки, только в функции get_embedding вам нужно выбирать вектор нужного значения (импользуйте model.disambiguate и model.sense_vector). Отдельные векторы усредните как и в предыдущем семинаре. Для вытаскивания пар (целевое слово, контекстые слова) вам нужно будет написать специальную функцию."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rK7Pwe_3QDN6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_disambiguate(word, context, model=adagram_model):\n",
        "  max_probs = model.disambiguate(word, context).argmax()\n",
        "  close_vectors = model.sense_vector(word, max_probs)\n",
        "  return close_vectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOR2o4hQiBfl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = [0,1,2,3,4,5,6,7,8,9]\n",
        "\n",
        "def get_words_in_context(words, window=3):\n",
        "\n",
        "    words_in_context = []\n",
        "    words_len = len(words)\n",
        "    for i, word in enumerate(words):\n",
        "      left_context = words[max(i-window, 0 if i<window else -sys.maxsize):i]\n",
        "      right_context = words[i+1:min(words_len, i+1+window)]\n",
        "      context = left_context+right_context\n",
        "      words_in_context.append([word, context])\n",
        "\n",
        "    return words_in_context"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byY6BUarWWHu",
        "colab_type": "code",
        "outputId": "08c64456-57e1-4874-a2c5-0a4d62461d45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "get_words_in_context(words)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, [1, 2, 3]],\n",
              " [1, [0, 2, 3, 4]],\n",
              " [2, [0, 1, 3, 4, 5]],\n",
              " [3, [0, 1, 2, 4, 5, 6]],\n",
              " [4, [1, 2, 3, 5, 6, 7]],\n",
              " [5, [2, 3, 4, 6, 7, 8]],\n",
              " [6, [3, 4, 5, 7, 8, 9]],\n",
              " [7, [4, 5, 6, 8, 9]],\n",
              " [8, [5, 6, 7, 9]],\n",
              " [9, [6, 7, 8]]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NU_1LCEAiMks",
        "colab_type": "text"
      },
      "source": [
        "Когда получиться такой же результат, добавьте эту функцию в get_embedding. Проходите циклом по результату работы get_words_in_context и поставляйте каждый элемент-список в model.disambiguate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pi0XaJfdiPMg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_embedding_adagram(text, model=adagram_model, window=3, dim=100):\n",
        "    \n",
        "    word2context = get_words_in_context(text, window)\n",
        "    vectors = np.zeros((len(word2context), dim))\n",
        "    \n",
        "    for i, (word, context) in enumerate(word2context):\n",
        "        try:\n",
        "            vectors[i] = get_disambiguate(word, context)\n",
        "        except (KeyError, ValueError):\n",
        "            continue\n",
        "    \n",
        "    if vectors.any():\n",
        "        vector = np.average(vectors, axis=0)\n",
        "    else:\n",
        "        vector = np.zeros((dim))\n",
        "        \n",
        "    return vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qesuirzLrG7M",
        "colab_type": "text"
      },
      "source": [
        "Обучите любую модель и оцените качество (кросс-валидацией)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiGmNJPvrrzq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dim = 100\n",
        "X_text_1 = np.zeros((len(data['text_1_norm']), dim))\n",
        "X_text_2 = np.zeros((len(data['text_2_norm']), dim))\n",
        "\n",
        "for i, text in enumerate(data['text_1_norm'].values):\n",
        "    X_text_1[i] = get_embedding_adagram(text, dim)\n",
        "    \n",
        "for i, text in enumerate(data['text_2_norm'].values):\n",
        "    X_text_2[i] = get_embedding_adagram(text, dim)\n",
        "\n",
        "X_text = np.concatenate([X_text_1, X_text_2], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pB0ALiksr6Fp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = data.label.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2M_mD1Enr9wz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rf(X_text, y=y, n_estimators=100, max_depth=10, min_samples_leaf=10,\n",
        "                             class_weight='balanced', random_state=42):\n",
        "  train_X, valid_X, train_y, valid_y = train_test_split(X_text, y, random_state=random_state)\n",
        "  clf_RF = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, min_samples_leaf=min_samples_leaf,\n",
        "                             class_weight=class_weight)\n",
        "  return clf_RF"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFpd6SBjrYsu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf = rf(X_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhKOHDz5re4K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cross_val(clf, X_text, y=y, scoring='f1_micro', cv=5):\n",
        "  cross = cross_val_score(clf, X_text, y, scoring=scoring, cv=cv)\n",
        "  print(f'Cross-validation: {cross}', f'F-score: {np.mean(cross)}', sep='\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPN2ePeFsU5-",
        "colab_type": "code",
        "outputId": "8ea41157-cc09-4f78-f239-505c9b97fca7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "cross_val(clf, X_text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross-validation: [0.42323651 0.45643154 0.50657439 0.39377163 0.41730104]\n",
            "F-score: 0.43946302172321217\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oy8AmxaCN1bc",
        "colab_type": "text"
      },
      "source": [
        "Я сначала попробовала сама обучить Адаграм на корпусе из Кругосвета, но получила кросс-валидацию несколько ниже, чем с Адаграмом с семинара, поэтому в итоге использовала именно его."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-6TchOGJgwd",
        "colab_type": "text"
      },
      "source": [
        "# Задание 2. \n",
        "\n",
        "## Реализовать алгоритм Леска и проверить его на реальном датасете"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWukKE9BRxPN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#функция, которая находит пересечения\n",
        "\n",
        "def get_overlap(synset, sentence):\n",
        "  definitions = set(normalize_en(synset.definition()))\n",
        "  sentence = set(normalize_en(sentence))\n",
        "\n",
        "  return len(definitions.intersection(sentence))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-C6LHjmJgwe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lesk(word, sentence):\n",
        "    bestsense = 0\n",
        "    maxoverlap = 0\n",
        "    synsets = wn.synsets(word)\n",
        "\n",
        "    if len(synsets) < 2: #если всего один синсет, этого уже достаточно\n",
        "      return bestsense\n",
        "    else:\n",
        "      for i, syns in enumerate(synsets):\n",
        "          overlap = get_overlap(syns, sentence)\n",
        "          if overlap > maxoverlap:\n",
        "            maxoverlap = overlap\n",
        "            bestsense = i\n",
        "        \n",
        "    return bestsense # индекс подходящего синсета"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QR9ltW0Jgwf",
        "colab_type": "text"
      },
      "source": [
        "Работать функция должна как-то так:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WNJ3EAQJgwg",
        "colab_type": "code",
        "outputId": "7482a4bd-57c5-4f45-fd76-9fd923ffbfa8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# на вход подается элемент результата работы уже написанной вами функции get_words_in_context\n",
        "lesk('day', 'some point or period in time') # для примера контекст совпадает с одним из определений\n",
        "# а на выходе индекс подходящего синсета"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2n6epHZqqeo",
        "colab_type": "text"
      },
      "source": [
        "Проверим еще"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOCGxHEzhj6M",
        "colab_type": "code",
        "outputId": "8a2e74ee-5a10-446b-cbe3-8d430f4d7e1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "lesk('word', 'a brief statement')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baNw6qx6hmts",
        "colab_type": "code",
        "outputId": "3126de1e-3006-40e2-9cbc-acbf9c5a0831",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "lesk('word', 'a verbal command for action')"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BmByis9iGn-",
        "colab_type": "code",
        "outputId": "3ef76a66-9e03-401b-a571-19003311c1e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "lesk('sun', 'first day of the week; observed as a day of rest and worship by most Christians')"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wgvnOKsqvpd",
        "colab_type": "text"
      },
      "source": [
        "Подготовим корпус"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81MVr7v6Jgwk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_wsd = []\n",
        "corpus = open('/content/drive/My Drive/wsd_wsi_seminar/corpus_wsd_50k.txt').read().split('\\n\\n')\n",
        "for sent in corpus:\n",
        "    corpus_wsd.append([s.split('\\t') for s in sent.split('\\n')])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06LVeRk0PM93",
        "colab_type": "code",
        "outputId": "a7d66653-0abc-43c2-b0ed-03c793c1b071",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "corpus_wsd[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['', 'how', 'How'],\n",
              " ['long%3:00:02::', 'long', 'long'],\n",
              " ['', 'have', 'has'],\n",
              " ['', 'it', 'it'],\n",
              " ['be%2:42:03::', 'be', 'been'],\n",
              " ['', 'since', 'since'],\n",
              " ['', 'you', 'you'],\n",
              " ['review%2:31:00::', 'review', 'reviewed'],\n",
              " ['', 'the', 'the'],\n",
              " ['objective%1:09:00::', 'objective', 'objectives'],\n",
              " ['', 'of', 'of'],\n",
              " ['', 'you', 'your'],\n",
              " ['benefit%1:21:00::', 'benefit', 'benefit'],\n",
              " ['', 'and', 'and'],\n",
              " ['service%1:04:07::', 'service', 'service'],\n",
              " ['program%1:09:01::', 'program', 'program'],\n",
              " ['', '?', '?']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJPRDCN2qy8E",
        "colab_type": "text"
      },
      "source": [
        "Напишем функцию, которая будет дизамбигуировать предложения, основываясь на алгоритме Леска"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKB_HP5DUY7e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def confirm_sense(word, sense, best_sense):\n",
        "    synsets = wn.synsets(word)\n",
        "    \n",
        "    if synsets:\n",
        "        best_synset = synsets[best_sense]\n",
        "        \n",
        "    true_sense = wn.lemma_from_key(sense).synset()\n",
        "\n",
        "    return best_synset == true_sense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFrBmFQaO_zs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def disambiguate_sentences(sentence):\n",
        "    words_num = 0 \n",
        "    correct_words_num = 0\n",
        "    words = [word[1] for word in sentence]\n",
        "    words_in_contexts = get_words_in_context(words)\n",
        "\n",
        "    for i, word_in_context in enumerate(words_in_contexts):\n",
        "        word = word_in_context[0]\n",
        "        context = word_in_context[1]\n",
        "        sense = sentence[i][0]\n",
        "        best_sense = lesk(word, context) #индекс максимального пересечения \n",
        "        \n",
        "        if sense:\n",
        "            words_num += 1\n",
        "            correct_words_num += confirm_sense(word, sense, best_sense)\n",
        "\n",
        "    return correct_words_num, words_num"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zcWdXgBkiua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval_disambiguation(corpus_wsd):\n",
        "  correct_words = []\n",
        "  words = []\n",
        "\n",
        "  for sent in corpus_wsd[:10000]:\n",
        "    correct_words_num, words_num = disambiguate_sentences(sent)\n",
        "    correct_words.append(correct_words_num)\n",
        "    words.append(words_num)\n",
        "\n",
        "  accuracy = sum(correct_words)/sum(words)\n",
        "\n",
        "  return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSlWJmDUnEzc",
        "colab_type": "code",
        "outputId": "d6c80586-a6aa-47e1-c20c-985990ddf748",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "%%time\n",
        "print(f'Точность {eval_disambiguation(corpus_wsd)}')"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Точность 0.5166790846194784\n",
            "CPU times: user 51min 5s, sys: 22 s, total: 51min 27s\n",
            "Wall time: 51min 29s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gJwTkdcjKf4",
        "colab_type": "text"
      },
      "source": [
        "Попробуем улучшить качество алгоритма, добавив к значениям синсетов еще и примеры"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9sLDnUBis_o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_new_overlap(synset, sentence):\n",
        "  definitions = set(normalize_en(synset.definition()))\n",
        "  sentence = set(normalize_en(sentence))\n",
        "\n",
        "  for example in synset.examples(): #добавим примеры вот тут\n",
        "    definitions.union(example)\n",
        "\n",
        "  return len(definitions.intersection(sentence))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hv9Vztjcin0l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def new_lesk(word, sentence):\n",
        "    bestsense = 0\n",
        "    maxoverlap = 0\n",
        "    synsets = wn.synsets(word)\n",
        "\n",
        "    if len(synsets) < 2: #если всего один синсет, этого уже достаточно\n",
        "      return bestsense\n",
        "    else:\n",
        "      for i, syns in enumerate(synsets):\n",
        "          overlap = get_new_overlap(syns, sentence)\n",
        "          if overlap > maxoverlap:\n",
        "            maxoverlap = overlap\n",
        "            bestsense = i\n",
        "        \n",
        "    return bestsense # индекс подходящего синсета"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-n3GfPoiveU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def new_disambiguate_sentences(sentence):\n",
        "    words_num = 0 \n",
        "    correct_words_num = 0\n",
        "    words = [word[1] for word in sentence]\n",
        "    words_in_contexts = get_words_in_context(words)\n",
        "\n",
        "    for i, word_in_context in enumerate(words_in_contexts):\n",
        "        word = word_in_context[0]\n",
        "        context = word_in_context[1]\n",
        "        sense = sentence[i][0]\n",
        "        best_sense = new_lesk(word, context) #индекс максимального пересечения \n",
        "        \n",
        "        if sense:\n",
        "            words_num += 1\n",
        "            correct_words_num += confirm_sense(word, sense, best_sense)\n",
        "\n",
        "    return correct_words_num, words_num"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAZU0wxcj8Jq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def new_eval_disambiguation(corpus_wsd):\n",
        "  correct_words = []\n",
        "  words = []\n",
        "\n",
        "  for sent in corpus_wsd[:10000]:\n",
        "    correct_words_num, words_num = new_disambiguate_sentences(sent)\n",
        "    correct_words.append(correct_words_num)\n",
        "    words.append(words_num)\n",
        "\n",
        "  accuracy = sum(correct_words)/sum(words)\n",
        "\n",
        "  return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZZyHTa2jzjf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ac83a18d-9fbd-4424-b957-89f9b30e410f"
      },
      "source": [
        "print(f'Точность {new_eval_disambiguation(corpus_wsd)}')"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Точность 0.5166790846194784\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PPgmJ0klHgp",
        "colab_type": "text"
      },
      "source": [
        "Прироста в качестве, к сожалению, достичь не получилось\n"
      ]
    }
  ]
}