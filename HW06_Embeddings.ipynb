{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "HW06_Embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wksmirnowa/compling_homeworks/blob/master/HW06_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnCgHKqcx7iZ",
        "colab_type": "text"
      },
      "source": [
        "# Векторные представления"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLcR3FCxmcyL",
        "colab_type": "text"
      },
      "source": [
        "## Импорты и настройка колаба для работы с fasttext с rusvectores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-2mxg0ZPygs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -c https://repo.anaconda.com/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-4.5.4-Linux-x86_64.sh\n",
        "!bash ./Miniconda3-4.5.4-Linux-x86_64.sh -b -f -p /usr/local"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PAVllvOPj3F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!conda update --all"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50WcKyo_3NwL",
        "colab_type": "code",
        "outputId": "bcc2020b-a58d-474b-d8a6-9ca85ff4783d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        }
      },
      "source": [
        "!pip install pymorphy2[fast]"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pymorphy2[fast]\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/33/fff9675c68b5f6c63ec8c6e6ff57827dda28a1fa5b2c2d727dffff92dd47/pymorphy2-0.8-py2.py3-none-any.whl (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 1.6MB/s \n",
            "\u001b[?25hCollecting dawg-python>=0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
            "Collecting pymorphy2-dicts<3.0,>=2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/51/2465fd4f72328ab50877b54777764d928da8cb15b74e2680fc1bd8cb3173/pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1MB 6.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2[fast]) (0.6.2)\n",
            "Collecting DAWG>=0.7.3; extra == \"fast\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b8/ef/91b619a399685f7a0a95a03628006ba814d96293bbbbed234ee66fbdefd9/DAWG-0.8.0.tar.gz (371kB)\n",
            "\u001b[K     |████████████████████████████████| 378kB 52.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: DAWG\n",
            "  Building wheel for DAWG (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for DAWG: filename=DAWG-0.8.0-cp36-cp36m-linux_x86_64.whl size=868528 sha256=bb6a786f08567a4b49650db9c5f2f8c6657309ab2f5c52e708391f5cd44c3500\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/1f/f0/a5b1f9d02e193c997d252c33d215f24dfd7a448bc0166b2a12\n",
            "Successfully built DAWG\n",
            "Installing collected packages: dawg-python, pymorphy2-dicts, DAWG, pymorphy2\n",
            "Successfully installed DAWG-0.8.0 dawg-python-0.7.2 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzodjWGa6ufa",
        "colab_type": "code",
        "outputId": "81f70883-b0c2-4013-9192-3f7ca4fd521c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "!pip install --upgrade gensim"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gensim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/b3/8358842ee8e430f7eb8f996bdd06c146a71712b9848ed32f949ad44b5adf/gensim-3.8.2-cp36-cp36m-manylinux1_x86_64.whl (24.2MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2MB 1.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.11.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.18.3)\n",
            "Requirement already satisfied, skipping upgrade: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim) (1.12.43)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
            "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.8.1->gensim) (0.9.5)\n",
            "Requirement already satisfied, skipping upgrade: botocore<1.16.0,>=1.15.43 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.8.1->gensim) (1.15.43)\n",
            "Requirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.8.1->gensim) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (2020.4.5.1)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.43->boto3->smart-open>=1.8.1->gensim) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.43->boto3->smart-open>=1.8.1->gensim) (0.15.2)\n",
            "Installing collected packages: gensim\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-3.8.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6g5f-F9x7ic",
        "colab_type": "code",
        "outputId": "2fed4341-3c34-4a24-cc47-1286b3e3d81c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "import pandas as pd\n",
        "from lxml import html\n",
        "import numpy as np\n",
        "import gensim\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "from sklearn.decomposition import TruncatedSVD, NMF\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from gensim.models import FastText\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from collections import Counter\n",
        "from string import punctuation\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "from pymystem3 import Mystem\n",
        "from scipy import spatial\n",
        "\n",
        "morph = MorphAnalyzer()\n",
        "mystem = Mystem()\n",
        "\n",
        "punct = punctuation+'«»—…“”*№–'\n",
        "stops = set(stopwords.words('russian')) | {'gt', }\n",
        "added_stops = {'весь', 'это', 'наш', 'оно', 'итак', 'т.п', 'т.е', 'мало', 'меньше', 'ещё', 'слишком', 'также',\n",
        "                   'ваш', 'б', 'хм', 'который', 'свой', 'не', 'мочь', 'однако', 'очень', 'п', 'благодаря', 'кроме'}\n",
        "stops = stops.union(added_stops)\n",
        "\n",
        "def normalize(text):\n",
        "    \n",
        "    words = [word.strip(punct) for word in text.lower().split()]\n",
        "    words = [morph.parse(word)[0].normal_form for word in words if word and word not in stops]\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "def tokenize(text):\n",
        "    \n",
        "    words = [word.strip(punct) for word in text.lower().split()]\n",
        "\n",
        "    return ' '.join(words)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Installing mystem to /root/.local/bin/mystem from http://download.cdn.yandex.net/mystem/mystem-3.1-linux-64bit.tar.gz\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duwAB8B95wch",
        "colab_type": "code",
        "outputId": "16f6fced-bac5-499e-c72d-9758275e7ac9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jX7kp1xZmp7E",
        "colab_type": "text"
      },
      "source": [
        "## Тексты"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZTnCQ4lx7jQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Корпус гуманитарных текстов для обучения w2v модели\n",
        "\n",
        "corpus_hum = open('/content/drive/My Drive/corpus_hum.txt').read().splitlines()\n",
        "\n",
        "corpus_hum_norm = [normalize(text) for text in corpus_hum]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2eeI4Dzx7jV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_hum_norm = [text for text in corpus_hum_norm if text]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRpX4o4P3EMp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Создание корпуса перефразирования \n",
        "\n",
        "corpus_xml = html.fromstring(open('/content/drive/My Drive/paraphraser/paraphrases.xml', 'rb').read())\n",
        "texts_1 = []\n",
        "texts_2 = []\n",
        "classes = []\n",
        "\n",
        "for p in corpus_xml.xpath('//paraphrase'):\n",
        "    texts_1.append(p.xpath('./value[@name=\"text_1\"]/text()')[0])\n",
        "    texts_2.append(p.xpath('./value[@name=\"text_2\"]/text()')[0])\n",
        "    classes.append(p.xpath('./value[@name=\"class\"]/text()')[0])\n",
        "    \n",
        "data = pd.DataFrame({'text_1':texts_1, 'text_2':texts_2, 'label':classes})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1AGPi5B3NmM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['text_1_norm'] = data['text_1'].apply(normalize)\n",
        "data['text_2_norm'] = data['text_2'].apply(normalize)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zIYvs_33P9F",
        "colab_type": "code",
        "outputId": "678ad752-fa98-45eb-fc8a-f6376569d1eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "source": [
        "data.head(2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_1</th>\n",
              "      <th>text_2</th>\n",
              "      <th>label</th>\n",
              "      <th>text_1_norm</th>\n",
              "      <th>text_2_norm</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Полицейским разрешат стрелять на поражение по ...</td>\n",
              "      <td>Полиции могут разрешить стрелять по хулиганам ...</td>\n",
              "      <td>0</td>\n",
              "      <td>полицейский разрешить стрелять поражение гражд...</td>\n",
              "      <td>полиция мочь разрешить стрелять хулиган травма...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Право полицейских на проникновение в жилище ре...</td>\n",
              "      <td>Правила внесудебного проникновения полицейских...</td>\n",
              "      <td>0</td>\n",
              "      <td>право полицейский проникновение жилища решить ...</td>\n",
              "      <td>правило внесудебный проникновение полицейский ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              text_1  ...                                        text_2_norm\n",
              "0  Полицейским разрешат стрелять на поражение по ...  ...  полиция мочь разрешить стрелять хулиган травма...\n",
              "1  Право полицейских на проникновение в жилище ре...  ...  правило внесудебный проникновение полицейский ...\n",
              "\n",
              "[2 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1XgSooyDPeI",
        "colab_type": "text"
      },
      "source": [
        "# Задание 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNbxOEg_mxKE",
        "colab_type": "text"
      },
      "source": [
        "## Векторизация"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2g9Ajm5392S",
        "colab_type": "text"
      },
      "source": [
        "1) Векторизуйте тексты с помощью Word2vec модели, обученной самостоятельно. Word2Vec нужно обучить на отдельном корпусе (не на парафразах). Можно взять данные из семинара или любые другие. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ER5brFmd-cV0",
        "colab_type": "text"
      },
      "source": [
        "2) Векторизуйте тексты с помощью и с помощью модели, взятой с rusvectores (любой). Я взяла fasttext, обученный на Araneum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mR-C1P8T1vuJ",
        "colab_type": "text"
      },
      "source": [
        "### Подготовка"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8wjvWnA1uqU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#соответствие тэгов майстема тэгам Udpipe \n",
        "\n",
        "mapping = {}\n",
        "\n",
        "for line in open('/content/drive/My Drive/ru-rnc.map.txt'):\n",
        "    ms, ud = line.strip('\\n').split()\n",
        "    mapping[ms] = ud"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iy5mPa8512xb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#проставление тегов нормализированным словам\n",
        "\n",
        "def normalize_mystem(text):\n",
        "    tokens = []\n",
        "    norm_words = mystem.analyze(text)\n",
        "    for norm_word in norm_words:\n",
        "        if 'analysis' not in norm_word:\n",
        "            continue\n",
        "            \n",
        "        if not len(norm_word['analysis']):\n",
        "            lemma = norm_word['text']\n",
        "            pos = 'UNKN'\n",
        "        else:\n",
        "            lemma = norm_word[\"analysis\"][0][\"lex\"].lower().strip()\n",
        "            pos = norm_word[\"analysis\"][0][\"gr\"].split(',')[0]\n",
        "            pos = pos.split('=')[0].strip()\n",
        "        pos = mapping[pos]\n",
        "        tokens.append(lemma+'_'+pos)\n",
        "\n",
        "    return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-h_dZry13yL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_embedding(text, model, dim):\n",
        "    text = text.split()\n",
        "    words = Counter(text)\n",
        "    total = len(text)\n",
        "    vectors = np.zeros((len(words), dim))\n",
        "    \n",
        "    for i,word in enumerate(words):\n",
        "        try:\n",
        "            v = model[word]\n",
        "            vectors[i] = v*(words[word]/total)\n",
        "        except (KeyError, ValueError):\n",
        "            continue\n",
        "    \n",
        "    if vectors.any():\n",
        "        vector = np.average(vectors, axis=0)\n",
        "    else:\n",
        "        vector = np.zeros((dim))\n",
        "    \n",
        "    return vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qr93OUFtD-jT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def logreg(X_text, y=y, random_state=None, C=1000, max_iter=1000):\n",
        "  train_X, valid_X, train_y, valid_y = train_test_split(X_text, y, random_state=random_state)\n",
        "  clf_LR = LogisticRegression(C=C, max_iter=max_iter)\n",
        "  return clf_LR"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HOavLlcFqcu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cross_val(clf, X_text, y=y, scoring='f1_micro', cv=5):\n",
        "  cross = cross_val_score(clf, X_text, y, scoring=scoring, cv=cv)\n",
        "  print(f'Cross-validation: {cross}', f'F-score: {np.mean(cross)}', sep='\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6NCuaMl1luI",
        "colab_type": "text"
      },
      "source": [
        "### Word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaFdogE71sgP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w2v = gensim.models.Word2Vec([text.split() for text in corpus_hum_norm], size=50, sg=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxheSnsL1tZV",
        "colab_type": "code",
        "outputId": "cbf2cccd-d6d0-4fb3-e59c-38ddde772e2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "dim = 50\n",
        "X_text_1_w2v = np.zeros((len(data['text_1_norm']), dim))\n",
        "X_text_2_w2v = np.zeros((len(data['text_2_norm']), dim))\n",
        "\n",
        "for i, text in enumerate(data['text_1_norm'].values):\n",
        "    X_text_1_w2v[i] = get_embedding(text, w2v, dim)\n",
        "    \n",
        "for i, text in enumerate(data['text_2_norm'].values):\n",
        "    X_text_2_w2v[i] = get_embedding(text, w2v, dim)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yFtta_a3zJ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_text_w2v = np.concatenate([X_text_1_w2v, X_text_2_w2v], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D89lT0Or5iM8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = data.label.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pjyfM66E2n6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf_w2v = logreg(X_text_w2v, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjuDUJE1Epb0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "71c2fb69-7f8c-42bd-db65-09c1efbb5383"
      },
      "source": [
        "cross_val(clf_w2v, X_text_w2v) #Word2Vec LogisticRegression"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross-validation: [0.42738589 0.46957123 0.4982699  0.43114187 0.42145329]\n",
            "F-score: 0.44956443500026316\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b612qcn17TH",
        "colab_type": "text"
      },
      "source": [
        "### Araneum Fasttext"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9HUwtxrm45J",
        "colab_type": "code",
        "outputId": "e464935a-6f6d-4dbd-a07f-f9de5da56947",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "!wget https://rusvectores.org/static/models/rusvectores4/fasttext/araneum_none_fasttextcbow_300_5_2018.tgz"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-25 12:18:19--  https://rusvectores.org/static/models/rusvectores4/fasttext/araneum_none_fasttextcbow_300_5_2018.tgz\n",
            "Resolving rusvectores.org (rusvectores.org)... 116.203.104.23\n",
            "Connecting to rusvectores.org (rusvectores.org)|116.203.104.23|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2691248108 (2.5G) [application/x-gzip]\n",
            "Saving to: ‘araneum_none_fasttextcbow_300_5_2018.tgz’\n",
            "\n",
            "araneum_none_fastte 100%[===================>]   2.51G  11.1MB/s    in 4m 1s   \n",
            "\n",
            "2020-04-25 12:22:21 (10.7 MB/s) - ‘araneum_none_fasttextcbow_300_5_2018.tgz’ saved [2691248108/2691248108]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDJWyMw9pptY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tarfile\n",
        "fasttext_file = tarfile.open('/content/araneum_none_fasttextcbow_300_5_2018.tgz', 'r')\n",
        "fasttext_file.extractall()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYcudsGim7E1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fasttext_araneum = gensim.models.KeyedVectors.load(\"araneum_none_fasttextcbow_300_5_2018.model\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jl5WBaZtnYfR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dim = 50\n",
        "X_text_1_fasttext_araneum = np.zeros((len(data['text_1_norm']), dim))\n",
        "X_text_2_fasttext_araneum = np.zeros((len(data['text_2_norm']), dim))\n",
        "\n",
        "for i, text in enumerate(data['text_1_norm'].values):\n",
        "    X_text_1_fasttext_araneum[i] = get_embedding(text, fasttext_araneum, dim)\n",
        "    \n",
        "for i, text in enumerate(data['text_2_norm'].values):\n",
        "    X_text_2_fasttext_araneum[i] = get_embedding(text, fasttext_araneum, dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4u2qMl1FnZRT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_text_fasttext_araneum = np.concatenate([X_text_1_fasttext_araneum, X_text_2_fasttext_araneum], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5s3qCwiiFT3Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf_fa = logreg(X_text_fasttext_araneum, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpiIwQ1yFYVU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d43b0d80-5013-484e-98d7-0d0bed3d574d"
      },
      "source": [
        "cross_val(clf_fa, X_text_fasttext_araneum) #Fasttext Areneum LogisticRegression"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross-validation: [0.40940526 0.40940526 0.40899654 0.40899654 0.40899654]\n",
            "F-score: 0.4091600262267464\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSb8Kfxv_t5L",
        "colab_type": "text"
      },
      "source": [
        "У обученного мною word2vec показатели чуть лучше, \n",
        "чем у предобученного на araneum фасттексте, но разница получилась совсем незначительная. Вероятно, это связано с тем, что предобученная на Araneum модель имела 5 млрд слов в обучении, поэтому она не сильно отстает от моей модели."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnQXk-iKDTih",
        "colab_type": "text"
      },
      "source": [
        "# Задание 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtmrdDzqDVa4",
        "colab_type": "text"
      },
      "source": [
        "1)\tПреобразуйте тексты в векторы в каждой паре с помощью SVD, SVD применяйте к данным напрямую."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wzCc-LdDa_i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cv = CountVectorizer(min_df=3, max_df=0.3, max_features=1000)\n",
        "X = cv.fit_transform(pd.concat([data['text_1_norm'], data['text_2_norm']]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNg84xbsD53P",
        "colab_type": "code",
        "outputId": "ccb4b6a0-aa22-4ecc-f5ff-32b84b67523b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14454, 1000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X38KjzXnFvAK",
        "colab_type": "code",
        "outputId": "ce9fb39c-df6f-46c3-b434-b71b0526c305",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "svd = TruncatedSVD(25)\n",
        "svd.fit(X)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TruncatedSVD(algorithm='randomized', n_components=25, n_iter=5,\n",
              "             random_state=None, tol=0.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gR7oAFpHFcQR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_text_1_svd = svd.transform(cv.fit_transform(data['text_1_norm']))\n",
        "X_text_2_svd = svd.transform(cv.fit_transform(data['text_2_norm']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LW9rePTHIVWW",
        "colab_type": "text"
      },
      "source": [
        "2)\tПреобразуйте тексты в векторы в каждой паре с помощью  NMF, NMF применяйте к данным напрямую"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5p_MlxMDITLy",
        "colab_type": "code",
        "outputId": "deda9e70-0afb-43f9-e4de-5f8dd8bb40a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "nmf = NMF(25)\n",
        "nmf.fit(X)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n",
              "    n_components=25, random_state=None, shuffle=False, solver='cd', tol=0.0001,\n",
              "    verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XJX0fTzI_gE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_text_1_nmf = nmf.transform(cv.fit_transform(data['text_1_norm']))\n",
        "X_text_2_nmf = nmf.transform(cv.fit_transform(data['text_2_norm']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woZHoPMWLUpL",
        "colab_type": "text"
      },
      "source": [
        "3)\tПреобразуйте тексты в векторы в каждой паре с помощью Word2Vec (свой и русвекторовский)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxd_UFbjL72Q",
        "colab_type": "text"
      },
      "source": [
        "4)\tПреобразуйте тексты в векторы в каждой паре с помощью Fastext."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWDmCTAhL86h",
        "colab_type": "code",
        "outputId": "c96fbc3e-250b-44bd-ff67-7a32d215f17a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "fasttext = gensim.models.FastText([text.split() for text in corpus_hum_norm], size=50, \n",
        "                                   min_n=4, max_n=8) \n",
        "dim = 50\n",
        "X_text_1_fasttext = np.zeros((len(data['text_1_norm']), dim))\n",
        "X_text_2_fasttext = np.zeros((len(data['text_2_norm']), dim))\n",
        "\n",
        "for i, text in enumerate(data['text_1_norm'].values):\n",
        "    X_text_1_fasttext[i] = get_embedding(text, fasttext, dim)\n",
        "    \n",
        "for i, text in enumerate(data['text_2_norm'].values):\n",
        "    X_text_2_fasttext[i] = get_embedding(text, fasttext, dim)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0BADHDNOTJz",
        "colab_type": "text"
      },
      "source": [
        "5)\tУ вас должно получиться 5 пар векторов для каждой строчки в датасете. Между векторами каждой пары вычислите косинусную близость (получится 5 чисел для каждой пары). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DDVYs66I3WP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_cosines(X_text_1, X_text_2):\n",
        "  cosines = []\n",
        "  for pair1, pair2 in zip(X_text_1, X_text_2):\n",
        "    if pair1.any() and pair2.any():\n",
        "      cosines.append(spatial.distance.cosine(pair1, pair2))\n",
        "    else:\n",
        "      cosines.append(0)\n",
        "  pairs = np.array(cosines)\n",
        "  return pairs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_eH5172KPrS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['svd'] = make_cosines(X_text_1_svd, X_text_2_svd)\n",
        "data['nmf'] = make_cosines(X_text_1_nmf, X_text_2_nmf)\n",
        "data['w2v'] = make_cosines(X_text_1_w2v, X_text_2_w2v)\n",
        "data['fasttext_araneum'] = make_cosines(X_text_1_fasttext_araneum, X_text_2_fasttext_araneum)\n",
        "data['fasttext'] = make_cosines(X_text_1_fasttext, X_text_2_fasttext)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D53utgxUKFCh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectors = data[['svd', 'nmf', 'w2v', 'fasttext_araneum', 'fasttext']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lE6Kvz-Chxmo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf_LR = LogisticRegression(C=1000, max_iter=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUl7gAo20Ni3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c13e1bf3-d951-4852-89c1-a52f09d50c9e"
      },
      "source": [
        "cross_val(clf_LR, vectors) #Оценим все модели"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross-validation: [0.54702628 0.59820194 0.6083045  0.46020761 0.49065744]\n",
            "F-score: 0.5408795531881291\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKmkZNsMXfFb",
        "colab_type": "text"
      },
      "source": [
        "Максимальный результат f-метрики, которого мне удалось добиться путем изменения параметров векторизации – 0.5408795531881291"
      ]
    }
  ]
}